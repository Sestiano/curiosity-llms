{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68fc8824",
   "metadata": {},
   "source": [
    "# WikiSpeedAI Analysis - Network Metrics\n",
    "\n",
    "This notebook analyzes JSON files generated by wikispeedai and calculates network metrics to compare Wikipedia vs LLM exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "388c9a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for JSON files with pattern: results/**/*.json\n",
      "Files found: 312\n",
      "Successfully loaded 312 records\n",
      "\n",
      "Data summary:\n",
      "Total records: 312\n",
      "\n",
      "Temperatures: ['0.3', '1.5']\n",
      "Starting pages: ['Albert_Einstein', 'Computer_Science']\n",
      "Personalities: ['baseline', 'busybody', 'dancer', 'hunter']\n",
      "Runs: []\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIRECTORY = 'results'\n",
    "\n",
    "def extract_metadata_from_path(file_path):\n",
    "    \"\"\"\n",
    "    Extract temperature, starting page, and personality from file path.\n",
    "    Expected structure: results/temp_X_Y_personality_NAME/StartingPage/file.json\n",
    "    \"\"\"\n",
    "    parts = Path(file_path).parts\n",
    "    \n",
    "    metadata = {\n",
    "        'temperature': None,\n",
    "        'run': None,\n",
    "        'starting_page': None,\n",
    "        'personality': None\n",
    "    }\n",
    "    \n",
    "    for i, part in enumerate(parts):\n",
    "        # Extract temperature and personality from folder name like \"temp_0_3_personality_baseline\"\n",
    "        if part.startswith('temp_'):\n",
    "            # Match pattern: temp_X_Y_personality_NAME\n",
    "            match = re.match(r'temp_(\\d+)_(\\d+)_personality_(.+)', part)\n",
    "            if match:\n",
    "                temp_int, temp_dec, personality = match.groups()\n",
    "                metadata['temperature'] = f\"{temp_int}.{temp_dec}\"\n",
    "                metadata['personality'] = personality\n",
    "        \n",
    "        # Starting page is the folder after temperature folder\n",
    "        elif metadata['temperature'] and not metadata['starting_page']:\n",
    "            # Check if this is not the filename\n",
    "            if not part.endswith('.json'):\n",
    "                metadata['starting_page'] = part\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def load_json_files_with_metadata(directory_pattern='results'):\n",
    "    \"\"\"\n",
    "    Load all JSON files with extracted metadata from path.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    \n",
    "    json_pattern = f\"{directory_pattern}/**/*.json\"\n",
    "    json_files = glob.glob(json_pattern, recursive=True)\n",
    "    \n",
    "    print(f\"Searching for JSON files with pattern: {json_pattern}\")\n",
    "    print(f\"Files found: {len(json_files)}\")\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                metadata = extract_metadata_from_path(json_file)\n",
    "                \n",
    "                if isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        if isinstance(item, dict):\n",
    "                            item.update(metadata)\n",
    "                            item['_file_path'] = json_file\n",
    "                            data_list.append(item)\n",
    "                elif isinstance(data, dict):\n",
    "                    data.update(metadata)\n",
    "                    data['_file_path'] = json_file\n",
    "                    data_list.append(data)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully loaded {len(data_list)} records\\n\")\n",
    "    return data_list\n",
    "\n",
    "all_data = load_json_files_with_metadata(DATA_DIRECTORY)\n",
    "\n",
    "# Display metadata summary\n",
    "if all_data:\n",
    "    df_temp = pd.DataFrame(all_data)\n",
    "    print(\"Data summary:\")\n",
    "    print(f\"Total records: {len(df_temp)}\")\n",
    "    print(f\"\\nTemperatures: {sorted(df_temp['temperature'].dropna().unique())}\")\n",
    "    print(f\"Starting pages: {sorted(df_temp['starting_page'].dropna().unique())}\")\n",
    "    print(f\"Personalities: {sorted(df_temp['personality'].dropna().unique())}\")\n",
    "    print(f\"Runs: {sorted(df_temp['run'].dropna().unique())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec0bd6",
   "metadata": {},
   "source": [
    "## Load Wikipedia Links\n",
    "\n",
    "Load the Wikipedia hyperlinks structure from the pickle file created by `wiki_links.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb983a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Wikipedia links for 18 pages\n",
      "Total links: 14179\n",
      "Average links per page: 787.7\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load Wikipedia links from pickle file\n",
    "wikipedia_links_file = 'wikipedia_links.pkl'\n",
    "\n",
    "try:\n",
    "    with open(wikipedia_links_file, 'rb') as f:\n",
    "        wikipedia_links = pickle.load(f)\n",
    "    \n",
    "    print(f\"Loaded Wikipedia links for {len(wikipedia_links)} pages\")\n",
    "    print(f\"Total links: {sum(len(links) for links in wikipedia_links.values())}\")\n",
    "    print(f\"Average links per page: {sum(len(links) for links in wikipedia_links.values()) / len(wikipedia_links):.1f}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{wikipedia_links_file}' not found!\")\n",
    "    print(\"Please run 'python wiki_links.py' first to download Wikipedia links.\")\n",
    "    wikipedia_links = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699aac7b",
   "metadata": {},
   "source": [
    "## Build Wikipedia Graph\n",
    "\n",
    "Build a graph using the actual Wikipedia hyperlink structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ac0e632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia Graph Statistics:\n",
      "Number of nodes (pages): 18\n",
      "Number of edges (links): 86\n",
      "Graph is connected: True\n",
      "Average degree: 9.56\n",
      "Density: 0.2810\n"
     ]
    }
   ],
   "source": [
    "def build_wikipedia_graph(wikipedia_links):\n",
    "    \"\"\"\n",
    "    Build a directed graph from Wikipedia hyperlink structure.\n",
    "    Args:\n",
    "        wikipedia_links: Dictionary mapping page titles to lists of linked pages\n",
    "    Returns:\n",
    "        networkx.DiGraph: Directed graph with Wikipedia links\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    if wikipedia_links is None:\n",
    "        return G\n",
    "    \n",
    "    for source_page, target_pages in wikipedia_links.items():\n",
    "        # Add source node\n",
    "        if source_page not in G.nodes():\n",
    "            G.add_node(source_page, page=source_page, source='wikipedia')\n",
    "        \n",
    "        # Add edges to all linked pages\n",
    "        for target_page in target_pages:\n",
    "            # Only add edges to pages that are also in our dataset\n",
    "            if target_page in wikipedia_links:\n",
    "                if target_page not in G.nodes():\n",
    "                    G.add_node(target_page, page=target_page, source='wikipedia')\n",
    "                \n",
    "                # Add edge\n",
    "                G.add_edge(source_page, target_page, weight=1, source='wikipedia')\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Build Wikipedia graph\n",
    "G_wiki = build_wikipedia_graph(wikipedia_links)\n",
    "\n",
    "print(f\"Wikipedia Graph Statistics:\")\n",
    "print(f\"Number of nodes (pages): {G_wiki.number_of_nodes()}\")\n",
    "print(f\"Number of edges (links): {G_wiki.number_of_edges()}\")\n",
    "print(f\"Graph is connected: {nx.is_weakly_connected(G_wiki) if G_wiki.number_of_nodes() > 0 else 'N/A'}\")\n",
    "\n",
    "if G_wiki.number_of_nodes() > 0:\n",
    "    print(f\"Average degree: {np.mean([d for n, d in G_wiki.degree()]):.2f}\")\n",
    "    print(f\"Density: {nx.density(G_wiki):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb65468",
   "metadata": {},
   "source": [
    "## Build Graphs by Temperature and Personality\n",
    "\n",
    "Build separate graphs for each combination of temperature and personality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e88a6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building graphs for each condition:\n",
      "  temp_0.3_personality_None: 22 nodes, 27 edges\n",
      "  temp_1.5_personality_None: 32 nodes, 41 edges\n",
      "\n",
      "Total graphs created: 2\n"
     ]
    }
   ],
   "source": [
    "def build_graph_from_data(data_list):\n",
    "    \"\"\"\n",
    "    Build a directed graph from navigation data.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    for data in data_list:\n",
    "        path = data.get('path', [])\n",
    "        \n",
    "        for i in range(len(path) - 1):\n",
    "            source = path[i]\n",
    "            target = path[i + 1]\n",
    "            \n",
    "            if source not in G.nodes():\n",
    "                G.add_node(source)\n",
    "            if target not in G.nodes():\n",
    "                G.add_node(target)\n",
    "            \n",
    "            if G.has_edge(source, target):\n",
    "                G[source][target]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(source, target, weight=1)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def group_data_by_conditions(data_list):\n",
    "    \"\"\"\n",
    "    Group data by temperature and personality.\n",
    "    Returns dict with keys like ('0.3', 'personality1').\n",
    "    \"\"\"\n",
    "    grouped = defaultdict(list)\n",
    "    \n",
    "    for data in data_list:\n",
    "        temp = data.get('temperature', 'unknown')\n",
    "        personality = data.get('personality', 'unknown')\n",
    "        key = (temp, personality)\n",
    "        grouped[key].append(data)\n",
    "    \n",
    "    return dict(grouped)\n",
    "\n",
    "# Group data and build graphs\n",
    "grouped_data = group_data_by_conditions(all_data)\n",
    "graphs = {}\n",
    "\n",
    "print(\"Building graphs for each condition:\")\n",
    "for (temp, personality), data in grouped_data.items():\n",
    "    graph_key = f\"temp_{temp}_personality_{personality}\"\n",
    "    graphs[graph_key] = build_graph_from_data(data)\n",
    "    print(f\"  {graph_key}: {graphs[graph_key].number_of_nodes()} nodes, {graphs[graph_key].number_of_edges()} edges\")\n",
    "\n",
    "print(f\"\\nTotal graphs created: {len(graphs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d87fe8",
   "metadata": {},
   "source": [
    "## Calculate Metrics for All Conditions\n",
    "\n",
    "Calculate network metrics for each temperature/personality combination and Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "111280ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating metrics for LLM graphs:\n",
      "  temp_0.3_personality_None: completed\n",
      "  temp_1.5_personality_None: completed\n",
      "\n",
      "Calculating metrics for Wikipedia:\n",
      "  Wikipedia: completed\n"
     ]
    }
   ],
   "source": [
    "def calculate_graph_metrics(G):\n",
    "    \"\"\"\n",
    "    Calculate all network metrics for a graph.\n",
    "    Returns dictionary with metric values.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    if G.number_of_nodes() == 0:\n",
    "        return metrics\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics['num_nodes'] = G.number_of_nodes()\n",
    "    metrics['num_edges'] = G.number_of_edges()\n",
    "    metrics['density'] = nx.density(G)\n",
    "    metrics['avg_degree'] = np.mean([d for n, d in G.degree()])\n",
    "    \n",
    "    # Degree distribution\n",
    "    in_degrees = [d for n, d in G.in_degree()]\n",
    "    out_degrees = [d for n, d in G.out_degree()]\n",
    "    metrics['avg_in_degree'] = np.mean(in_degrees)\n",
    "    metrics['avg_out_degree'] = np.mean(out_degrees)\n",
    "    metrics['max_in_degree'] = max(in_degrees)\n",
    "    metrics['max_out_degree'] = max(out_degrees)\n",
    "    \n",
    "    # Clustering coefficient\n",
    "    G_undirected = G.to_undirected()\n",
    "    clustering = nx.clustering(G_undirected)\n",
    "    metrics['avg_clustering'] = np.mean(list(clustering.values()))\n",
    "    \n",
    "    # Shortest path\n",
    "    if nx.is_weakly_connected(G):\n",
    "        try:\n",
    "            metrics['avg_shortest_path'] = nx.average_shortest_path_length(G_undirected)\n",
    "        except:\n",
    "            metrics['avg_shortest_path'] = None\n",
    "    else:\n",
    "        components = list(nx.weakly_connected_components(G))\n",
    "        if components:\n",
    "            largest = max(components, key=len)\n",
    "            subgraph = G.subgraph(largest).to_undirected()\n",
    "            try:\n",
    "                metrics['avg_shortest_path'] = nx.average_shortest_path_length(subgraph)\n",
    "            except:\n",
    "                metrics['avg_shortest_path'] = None\n",
    "        else:\n",
    "            metrics['avg_shortest_path'] = None\n",
    "    \n",
    "    # PageRank\n",
    "    try:\n",
    "        pagerank = nx.pagerank(G, alpha=0.85, max_iter=100)\n",
    "        pr_values = list(pagerank.values())\n",
    "        metrics['mean_pagerank'] = np.mean(pr_values)\n",
    "        metrics['max_pagerank'] = max(pr_values)\n",
    "    except:\n",
    "        metrics['mean_pagerank'] = None\n",
    "        metrics['max_pagerank'] = None\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics for all graphs\n",
    "all_metrics = {}\n",
    "\n",
    "print(\"Calculating metrics for LLM graphs:\")\n",
    "for graph_key, G in graphs.items():\n",
    "    all_metrics[graph_key] = calculate_graph_metrics(G)\n",
    "    print(f\"  {graph_key}: completed\")\n",
    "\n",
    "# Calculate metrics for Wikipedia\n",
    "print(\"\\nCalculating metrics for Wikipedia:\")\n",
    "all_metrics['wikipedia'] = calculate_graph_metrics(G_wiki)\n",
    "print(\"  Wikipedia: completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d835689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison DataFrame created with 3 rows\n"
     ]
    }
   ],
   "source": [
    "# Create comparison DataFrame\n",
    "rows = []\n",
    "\n",
    "for graph_key, metrics in all_metrics.items():\n",
    "    if 'temp_' in graph_key:\n",
    "        # Parse LLM graph key\n",
    "        parts = graph_key.split('_')\n",
    "        temp_idx = parts.index('temp') + 1\n",
    "        pers_idx = parts.index('personality') + 1\n",
    "        \n",
    "        row = {\n",
    "            'Source': 'LLM',\n",
    "            'Temperature': parts[temp_idx],\n",
    "            'Personality': '_'.join(parts[pers_idx:]),\n",
    "            **metrics\n",
    "        }\n",
    "    else:\n",
    "        # Wikipedia row\n",
    "        row = {\n",
    "            'Source': 'Wikipedia',\n",
    "            'Temperature': 'N/A',\n",
    "            'Personality': 'N/A',\n",
    "            **metrics\n",
    "        }\n",
    "    \n",
    "    rows.append(row)\n",
    "\n",
    "df_comparison = pd.DataFrame(rows)\n",
    "\n",
    "# Reorder columns\n",
    "first_cols = ['Source', 'Temperature', 'Personality', 'num_nodes', 'num_edges', 'density', \n",
    "              'avg_degree', 'avg_in_degree', 'avg_out_degree', 'avg_clustering', \n",
    "              'avg_shortest_path', 'mean_pagerank', 'max_pagerank']\n",
    "df_comparison = df_comparison[first_cols + [c for c in df_comparison.columns if c not in first_cols]]\n",
    "\n",
    "print(\"\\nComparison DataFrame created with\", len(df_comparison), \"rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5bcd01ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL RESULTS\n",
      "========================================================================================================================\n",
      "   Source Temperature Personality  num_nodes  num_edges  density avg_degree avg_in_degree avg_out_degree avg_clustering avg_shortest_path mean_pagerank max_pagerank  max_in_degree  max_out_degree\n",
      "      LLM         0.3        None         22         27 0.058442   2.454545      1.227273       1.227273       0.284632          4.012987      0.045455     0.160142              4               7\n",
      "      LLM         1.5        None         32         41 0.041331   2.562500      1.281250       1.281250       0.126811          3.635081      0.031250     0.164228              6               9\n",
      "Wikipedia         N/A         N/A         18         86 0.281046   9.555556      4.777778       4.777778       0.556614          1.843137      0.055556     0.124733             10               9\n",
      "========================================================================================================================\n",
      "\n",
      "Results saved to: metrics_comparison.json\n",
      "\n",
      "Analysis completed!\n"
     ]
    }
   ],
   "source": [
    "# Save final comparison table\n",
    "output_file = 'metrics_comparison.json'\n",
    "df_comparison.to_json(output_file, orient='records', indent=2)\n",
    "\n",
    "print(\"\\nFINAL RESULTS\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Format numeric columns for better readability\n",
    "df_display = df_comparison.copy()\n",
    "for col in df_display.columns:\n",
    "    if col not in ['Source', 'Temperature', 'Personality']:\n",
    "        if df_display[col].dtype in ['float64', 'float32']:\n",
    "            df_display[col] = df_display[col].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else \"N/A\")\n",
    "\n",
    "# Display with better formatting\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.max_colwidth', 15)\n",
    "\n",
    "print(df_display.to_string(index=False))\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(f\"\\nResults saved to: {output_file}\")\n",
    "print(\"\\nAnalysis completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki_nav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
