{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68fc8824",
   "metadata": {},
   "source": [
    "# WikiSpeedAI Analysis - Network Metrics\n",
    "\n",
    "This notebook analyzes JSON files generated by wikispeedai and calculates network metrics to compare Wikipedia vs LLM exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c9a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "def load_json_files(directory_pattern='*'):\n",
    "    \"\"\"\n",
    "    Load all JSON files from directories matching the pattern.\n",
    "    Args:\n",
    "        directory_pattern: Pattern for directory names (default: '*' = all)\n",
    "    Returns:\n",
    "        List of dictionaries containing data from JSON files\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    \n",
    "    # Search for all JSON files in directories matching the pattern\n",
    "    json_pattern = f\"{directory_pattern}/*.json\"\n",
    "    json_files = glob.glob(json_pattern)\n",
    "    \n",
    "    print(f\"Searching for JSON files with pattern: {json_pattern}\")\n",
    "    print(f\"Files found: {len(json_files)}\")\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                # Add file information\n",
    "                data['_file_path'] = json_file\n",
    "                data['_file_name'] = os.path.basename(json_file)\n",
    "                data['_directory'] = os.path.dirname(json_file)\n",
    "                data_list.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully loaded {len(data_list)} JSON files\\n\")\n",
    "    return data_list\n",
    "\n",
    "# Load all JSON files from directories\n",
    "all_data = load_json_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1699362c",
   "metadata": {},
   "source": [
    "### Build a directed graph from a list of navigation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc6def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_from_data(data_list, source_type='llm'):\n",
    "    \"\"\"\n",
    "    Build a directed graph from a list of navigation data.\n",
    "    Args:\n",
    "        data_list: List of dictionaries with navigation results\n",
    "        source_type: Source type ('llm', 'wikipedia', etc.')\n",
    "    Returns:\n",
    "        networkx.DiGraph: Directed graph with paths\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    for data in data_list:\n",
    "        path = data.get('path', [])\n",
    "        \n",
    "        # Add nodes and edges from the path\n",
    "        for i in range(len(path) - 1):\n",
    "            source = path[i]\n",
    "            target = path[i + 1]\n",
    "            \n",
    "            # Add nodes with attributes\n",
    "            if source not in G.nodes():\n",
    "                G.add_node(source, page=source, source=source_type)\n",
    "            if target not in G.nodes():\n",
    "                G.add_node(target, page=target, source=source_type)\n",
    "            \n",
    "            # Add edge (or increment weight if it already exists)\n",
    "            if G.has_edge(source, target):\n",
    "                G[source][target]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(source, target, weight=1, source=source_type)\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Build the graph from all data\n",
    "G_llm = build_graph_from_data(all_data, source_type='llm')\n",
    "\n",
    "print(f\"LLM Graph Statistics:\")\n",
    "print(f\"  - Number of nodes (pages): {G_llm.number_of_nodes()}\")\n",
    "print(f\"  - Number of edges (links): {G_llm.number_of_edges()}\")\n",
    "print(f\"  - Graph is connected: {nx.is_weakly_connected(G_llm)}\")\n",
    "\n",
    "if G_llm.number_of_nodes() > 0:\n",
    "    print(f\"  - Average degree: {np.mean([d for n, d in G_llm.degree()]):.2f}\")\n",
    "    print(f\"  - Density: {nx.density(G_llm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba1f67",
   "metadata": {},
   "source": [
    "###  Calculate the average shortest path length of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cef5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_shortest_path(G):\n",
    "    \"\"\"\n",
    "    Calculate the average shortest path length of the graph.\n",
    "    If the graph is not connected, calculate for each connected component.\n",
    "    \"\"\"\n",
    "    if G.number_of_nodes() == 0:\n",
    "        return None\n",
    "    \n",
    "    # For directed graphs, use weakly connected components\n",
    "    if nx.is_weakly_connected(G):\n",
    "        try:\n",
    "            # Convert to undirected graph for calculation\n",
    "            G_undirected = G.to_undirected()\n",
    "            avg_path = nx.average_shortest_path_length(G_undirected)\n",
    "            return avg_path\n",
    "        except:\n",
    "            return None\n",
    "    else:\n",
    "        # Calculate for each weakly connected component\n",
    "        components = list(nx.weakly_connected_components(G))\n",
    "        path_lengths = []\n",
    "        \n",
    "        for comp in components:\n",
    "            if len(comp) > 1:\n",
    "                subgraph = G.subgraph(comp).to_undirected()\n",
    "                try:\n",
    "                    avg_path = nx.average_shortest_path_length(subgraph)\n",
    "                    path_lengths.append(avg_path)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        if path_lengths:\n",
    "            return np.mean(path_lengths)\n",
    "        return None\n",
    "\n",
    "# Calculate average shortest path length\n",
    "avg_shortest_path = calculate_average_shortest_path(G_llm)\n",
    "\n",
    "print(\"Average Shortest Path Length:\")\n",
    "if avg_shortest_path is not None:\n",
    "    print(f\"  - LLM Exploration: {avg_shortest_path:.4f}\")\n",
    "else:\n",
    "    print(\"  - Not calculable (disconnected graph or too small)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8479732e",
   "metadata": {},
   "source": [
    "### Calculate the average clustering coefficient and distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clustering_metrics(G):\n",
    "    \"\"\"\n",
    "    Calculate the average clustering coefficient and distribution.\n",
    "    \"\"\"\n",
    "    if G.number_of_nodes() == 0:\n",
    "        return None, None\n",
    "    \n",
    "    # Convert to undirected graph for clustering coefficient\n",
    "    G_undirected = G.to_undirected()\n",
    "    \n",
    "    # Calculate clustering coefficient for each node\n",
    "    clustering_coeffs = nx.clustering(G_undirected)\n",
    "    \n",
    "    # Average\n",
    "    avg_clustering = np.mean(list(clustering_coeffs.values()))\n",
    "    \n",
    "    return avg_clustering, clustering_coeffs\n",
    "\n",
    "# Calculate clustering coefficient\n",
    "avg_clustering, clustering_dict = calculate_clustering_metrics(G_llm)\n",
    "\n",
    "print(\"Local Clustering Coefficient:\")\n",
    "if avg_clustering is not None:\n",
    "    print(f\"  - Average clustering coefficient: {avg_clustering:.4f}\")\n",
    "    print(f\"  - Min clustering: {min(clustering_dict.values()):.4f}\")\n",
    "    print(f\"  - Max clustering: {max(clustering_dict.values()):.4f}\")\n",
    "    print(f\"  - Median clustering: {np.median(list(clustering_dict.values())):.4f}\")\n",
    "    \n",
    "    # Show nodes with highest clustering\n",
    "    top_clustered = sorted(clustering_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"\\n  Top 10 nodes with highest clustering:\")\n",
    "    for node, coeff in top_clustered:\n",
    "        print(f\"    - {node}: {coeff:.4f}\")\n",
    "else:\n",
    "    print(\"  - Not calculable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7106d3",
   "metadata": {},
   "source": [
    "### Calculate metrics related to edges and graph density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaf68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_graph_density_metrics(G):\n",
    "    \"\"\"\n",
    "    Calculate metrics related to edges and graph density.\n",
    "    \"\"\"\n",
    "    if G.number_of_nodes() == 0:\n",
    "        return {}\n",
    "    \n",
    "    num_nodes = G.number_of_nodes()\n",
    "    num_edges = G.number_of_edges()\n",
    "    density = nx.density(G)\n",
    "    \n",
    "    # Calculate the maximum possible number of edges for a directed graph\n",
    "    max_possible_edges = num_nodes * (num_nodes - 1)\n",
    "    \n",
    "    # Degree distribution (in-degree and out-degree)\n",
    "    in_degrees = [d for n, d in G.in_degree()]\n",
    "    out_degrees = [d for n, d in G.out_degree()]\n",
    "    \n",
    "    metrics = {\n",
    "        'num_nodes': num_nodes,\n",
    "        'num_edges': num_edges,\n",
    "        'density': density,\n",
    "        'max_possible_edges': max_possible_edges,\n",
    "        'avg_in_degree': np.mean(in_degrees) if in_degrees else 0,\n",
    "        'avg_out_degree': np.mean(out_degrees) if out_degrees else 0,\n",
    "        'max_in_degree': max(in_degrees) if in_degrees else 0,\n",
    "        'max_out_degree': max(out_degrees) if out_degrees else 0,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate density metrics\n",
    "density_metrics = calculate_graph_density_metrics(G_llm)\n",
    "\n",
    "print(\"Edge and Density Metrics:\")\n",
    "print(f\"  - Number of nodes: {density_metrics['num_nodes']}\")\n",
    "print(f\"  - Number of edges: {density_metrics['num_edges']}\")\n",
    "print(f\"  - Graph density: {density_metrics['density']:.6f}\")\n",
    "print(f\"  - Max possible edges: {density_metrics['max_possible_edges']}\")\n",
    "print(f\"\\n  - Average in-degree: {density_metrics['avg_in_degree']:.2f}\")\n",
    "print(f\"  - Average out-degree: {density_metrics['avg_out_degree']:.2f}\")\n",
    "print(f\"  - Max in-degree: {density_metrics['max_in_degree']}\")\n",
    "print(f\"  - Max out-degree: {density_metrics['max_out_degree']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e19fcc",
   "metadata": {},
   "source": [
    "### Calculate PageRank for all nodes in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c428b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pagerank(G, alpha=0.85, num_walks=10000, walk_length=100):\n",
    "    \"\"\"\n",
    "    Calculate PageRank using random walker simulation (Monte Carlo method).\n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        alpha: Probability of following a link (vs random jump, default 0.85)\n",
    "        num_walks: Number of random walks to simulate (default 10000)\n",
    "        walk_length: Length of each random walk (default 100)\n",
    "    Returns:\n",
    "        dict: PageRank for each node (normalized visit counts)\n",
    "    \"\"\"\n",
    "    if G.number_of_nodes() == 0:\n",
    "        return {}\n",
    "    \n",
    "    import random\n",
    "    \n",
    "    visit_counts = {node: 0 for node in G.nodes()}\n",
    "    nodes_list = list(G.nodes())\n",
    "    \n",
    "    for _ in range(num_walks):\n",
    "        current_node = random.choice(nodes_list)\n",
    "        \n",
    "        for _ in range(walk_length):\n",
    "            visit_counts[current_node] += 1\n",
    "\n",
    "            if random.random() > alpha:\n",
    "                current_node = random.choice(nodes_list)\n",
    "            else:\n",
    "                out_edges = list(G.successors(current_node))\n",
    "                if out_edges:\n",
    "                    current_node = random.choice(out_edges)\n",
    "                else:\n",
    "                    current_node = random.choice(nodes_list)\n",
    "\n",
    "    total_visits = sum(visit_counts.values())\n",
    "    pagerank = {node: count / total_visits for node, count in visit_counts.items()}\n",
    "    \n",
    "    return pagerank\n",
    "\n",
    "pagerank_scores = calculate_pagerank(G_llm)\n",
    "\n",
    "print(\"PageRank Distribution (Random Walker Method):\")\n",
    "if pagerank_scores:\n",
    "    pr_values = list(pagerank_scores.values())\n",
    "    print(f\"  - Mean PageRank: {np.mean(pr_values):.6f}\")\n",
    "    print(f\"  - Median PageRank: {np.median(pr_values):.6f}\")\n",
    "    print(f\"  - Standard deviation: {np.std(pr_values):.6f}\")\n",
    "    print(f\"  - Min PageRank: {min(pr_values):.6f}\")\n",
    "    print(f\"  - Max PageRank: {max(pr_values):.6f}\")\n",
    "    \n",
    "    top_pages = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"\\n  Top 10 pages by PageRank:\")\n",
    "    for i, (page, score) in enumerate(top_pages, 1):\n",
    "        print(f\"    {i}. {page}: {score:.6f}\")\n",
    "else:\n",
    "    print(\"  - Not calculable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6041f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pagerank_scores:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histogram of PageRank distribution\n",
    "    pr_values = list(pagerank_scores.values())\n",
    "    axes[0].hist(pr_values, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    axes[0].set_xlabel('PageRank Score', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].set_title('PageRank Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Log-scale histogram to better see the tail\n",
    "    axes[1].hist(pr_values, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "    axes[1].set_xlabel('PageRank Score', fontsize=12)\n",
    "    axes[1].set_ylabel('Frequency (log scale)', fontsize=12)\n",
    "    axes[1].set_title('PageRank Distribution (logarithmic scale)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"PageRank distribution plots generated\")\n",
    "else:\n",
    "    print(\"No PageRank data available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf27ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if G_llm.number_of_nodes() > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # In-degree distribution\n",
    "    in_degrees = [d for n, d in G_llm.in_degree()]\n",
    "    axes[0].hist(in_degrees, bins=30, edgecolor='black', alpha=0.7, color='lightgreen')\n",
    "    axes[0].set_xlabel('In-Degree', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].set_title('In-Degree Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Out-degree distribution\n",
    "    out_degrees = [d for n, d in G_llm.out_degree()]\n",
    "    axes[1].hist(out_degrees, bins=30, edgecolor='black', alpha=0.7, color='lightcoral')\n",
    "    axes[1].set_xlabel('Out-Degree', fontsize=12)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[1].set_title('Out-Degree Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Degree distribution plots generated\")\n",
    "else:\n",
    "    print(\"Empty graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fac6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "if G_llm.number_of_nodes() > 0 and G_llm.number_of_nodes() < 100:\n",
    "    # Visualize only if the graph is not too large\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Use spring layout to position nodes\n",
    "    pos = nx.spring_layout(G_llm, k=2, iterations=50, seed=42)\n",
    "    \n",
    "    # Node size based on PageRank\n",
    "    if pagerank_scores:\n",
    "        node_sizes = [pagerank_scores.get(node, 0) * 10000 for node in G_llm.nodes()]\n",
    "    else:\n",
    "        node_sizes = [100 for _ in G_llm.nodes()]\n",
    "    \n",
    "    # Node color based on clustering coefficient\n",
    "    if clustering_dict:\n",
    "        node_colors = [clustering_dict.get(node, 0) for node in G_llm.nodes()]\n",
    "    else:\n",
    "        node_colors = ['skyblue' for _ in G_llm.nodes()]\n",
    "    \n",
    "    # Draw the graph\n",
    "    nx.draw_networkx_nodes(G_llm, pos, node_size=node_sizes, \n",
    "                          node_color=node_colors, cmap='YlOrRd',\n",
    "                          alpha=0.8, edgecolors='black', linewidths=1.5)\n",
    "    \n",
    "    nx.draw_networkx_edges(G_llm, pos, alpha=0.3, edge_color='gray',\n",
    "                          arrows=True, arrowsize=15, width=1.5,\n",
    "                          connectionstyle='arc3,rad=0.1')\n",
    "    \n",
    "    nx.draw_networkx_labels(G_llm, pos, font_size=8, font_weight='bold')\n",
    "    \n",
    "    plt.title('LLM Exploration Graph\\n(node size = PageRank, color = Clustering Coefficient)', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Graph visualized ({G_llm.number_of_nodes()} nodes)\")\n",
    "elif G_llm.number_of_nodes() >= 100:\n",
    "    print(f\"Graph too large for visualization ({G_llm.number_of_nodes()} nodes)\")\n",
    "    print(\"   Consider filtering or aggregating the data for better visualization\")\n",
    "else:\n",
    "    print(\"Empty graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bb9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with all metrics\n",
    "metrics_summary = {\n",
    "    'Metric': [\n",
    "        'Number of Nodes (Pages)',\n",
    "        'Number of Edges (Links)',\n",
    "        'Graph Density',\n",
    "        'Average Shortest Path Length',\n",
    "        'Average Clustering Coefficient',\n",
    "        'Average In-Degree',\n",
    "        'Average Out-Degree',\n",
    "        'Mean PageRank',\n",
    "        'Median PageRank',\n",
    "        'Max PageRank'\n",
    "    ],\n",
    "    'Value': [\n",
    "        density_metrics['num_nodes'],\n",
    "        density_metrics['num_edges'],\n",
    "        f\"{density_metrics['density']:.6f}\",\n",
    "        f\"{avg_shortest_path:.4f}\" if avg_shortest_path else \"N/A\",\n",
    "        f\"{avg_clustering:.4f}\" if avg_clustering else \"N/A\",\n",
    "        f\"{density_metrics['avg_in_degree']:.2f}\",\n",
    "        f\"{density_metrics['avg_out_degree']:.2f}\",\n",
    "        f\"{np.mean(list(pagerank_scores.values())):.6f}\" if pagerank_scores else \"N/A\",\n",
    "        f\"{np.median(list(pagerank_scores.values())):.6f}\" if pagerank_scores else \"N/A\",\n",
    "        f\"{max(pagerank_scores.values()):.6f}\" if pagerank_scores else \"N/A\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_summary)\n",
    "\n",
    "print(\"NETWORK METRICS SUMMARY - LLM EXPLORATION\")\n",
    "print(df_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd01ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to a CSV file\n",
    "output_file = 'network_metrics_summary.csv'\n",
    "df_metrics.to_csv(output_file, index=False)\n",
    "print(f\"Metrics saved to: {output_file}\")\n",
    "\n",
    "# Save PageRank scores\n",
    "if pagerank_scores:\n",
    "    df_pagerank = pd.DataFrame(list(pagerank_scores.items()), \n",
    "                               columns=['Page', 'PageRank'])\n",
    "    df_pagerank = df_pagerank.sort_values('PageRank', ascending=False)\n",
    "    pagerank_file = 'pagerank_scores.csv'\n",
    "    df_pagerank.to_csv(pagerank_file, index=False)\n",
    "    print(f\"PageRank scores saved to: {pagerank_file}\")\n",
    "\n",
    "# Save clustering coefficients\n",
    "if clustering_dict:\n",
    "    df_clustering = pd.DataFrame(list(clustering_dict.items()),\n",
    "                                 columns=['Page', 'Clustering_Coefficient'])\n",
    "    df_clustering = df_clustering.sort_values('Clustering_Coefficient', ascending=False)\n",
    "    clustering_file = 'clustering_coefficients.csv'\n",
    "    df_clustering.to_csv(clustering_file, index=False)\n",
    "    print(f\"Clustering coefficients saved to: {clustering_file}\")\n",
    "\n",
    "print(\"\\nAnalysis completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
